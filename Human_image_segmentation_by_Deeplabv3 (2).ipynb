{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEvWHtGh4YDp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import imageio\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from tensorflow.keras import backend as k\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from sklearn.metrics import f1_score, jaccard_score, precision_score, recall_score, accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.layers import (Dense, Conv2D, BatchNormalization, Activation, MaxPool2D,\n",
        "                                        Conv2DTranspose, Concatenate, Input, Flatten,\n",
        "                                    AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Reshape)\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.metrics import Recall, Precision, IoU\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from albumentations import HorizontalFlip, GridDistortion, OpticalDistortion, ChannelShuffle, CoarseDropout, CenterCrop, Crop, Rotate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpLLegeZ4ZV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7191b4-b4b1-4bae-fa47-583bc30e7545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/people_seg.zip\", 'r')\n",
        "tqdm(zip_ref.extractall(\"/content/people_segmentation\"))\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H0KBn_44uD-"
      },
      "outputs": [],
      "source": [
        "H = 512\n",
        "W = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSkY4ZTU4ZYY"
      },
      "outputs": [],
      "source": [
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVltnXJa4ZbI"
      },
      "outputs": [],
      "source": [
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-ydOFI-4Zdw"
      },
      "outputs": [],
      "source": [
        "def load_data(path, split=0.1):\n",
        "    images = sorted(glob(os.path.join(path, \"images\", \"*.jpg\")))\n",
        "    masks = sorted(glob(os.path.join(path, \"masks\", \"*.png\")))\n",
        "\n",
        "    #for image, mask in zip(images, masks):\n",
        "        #print(image, mask)\n",
        "            #image_1 = cv2.imread(image)\n",
        "\n",
        "        #cv2.imwrite(r\"E:\\python\\segmentation\\Computer Vision\\UNET\\data\\people_segmentation\\my_image.png\", image_1)\n",
        "        #mask_1 = cv2.imread(mask)\n",
        "        #cv2.imwrite(r\"E:\\python\\segmentation\\Computer Vision\\UNET\\data\\people_segmentation\\mask_1.png\", mask_1 * 255)\n",
        "\n",
        "    split_size = int(len(images) * split)\n",
        "\n",
        "    X_train, X_test = train_test_split(images, test_size=split_size, random_state=42)\n",
        "    y_train, y_test = train_test_split(masks, test_size=split_size, random_state=42)\n",
        "\n",
        "    return (X_train, y_train), (X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBXGqNiR4ZgY"
      },
      "outputs": [],
      "source": [
        "def augment_data(images, masks, save_aug_path, augment=True):\n",
        "    H = 512\n",
        "    W = 512\n",
        "\n",
        "    for x, y in tqdm(zip(images, masks), total=len(images)):\n",
        "        #print(x, y)\n",
        "\n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "        #print(name)\n",
        "\n",
        "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        y = cv2.imread(y, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if augment == True:\n",
        "            aug = HorizontalFlip(p=1.0)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x1 = augmented[\"image\"]\n",
        "            y1 = augmented[\"mask\"]\n",
        "\n",
        "            x2 = cv2.cvtColor(x, cv2.COLOR_RGB2GRAY)\n",
        "            y2 = y\n",
        "\n",
        "            aug = ChannelShuffle(p=1)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x3 = augmented[\"image\"]\n",
        "            y3 = augmented[\"mask\"]\n",
        "\n",
        "            aug = CoarseDropout(p=1, min_holes=3, max_holes=10, max_height=32, max_width=32)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x4 = augmented[\"image\"]\n",
        "            y4 = augmented[\"mask\"]\n",
        "\n",
        "            aug = Rotate(limit=5, p=1.0)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x5 = augmented[\"image\"]\n",
        "            y5 = augmented[\"mask\"]\n",
        "\n",
        "            X = [x, x1, x2, x3, x4, x5]\n",
        "            Y = [y, y1, y2, y3, y4, y5]\n",
        "\n",
        "        else:\n",
        "            X = [x]\n",
        "            Y = [y]\n",
        "\n",
        "        index = 0\n",
        "        for i, m in zip(X, Y):\n",
        "\n",
        "            try:\n",
        "                aug = CenterCrop(H, W, p=1.0)\n",
        "                augmented = aug(image=i, mask=m)\n",
        "                i = augmented[\"image\"]\n",
        "                m = augmented[\"mask\"]\n",
        "\n",
        "            except Exception as e:\n",
        "                i = cv2.resize(i, (W, H))\n",
        "                m = cv2.resize(m, (W, H))\n",
        "\n",
        "            tem_image_name = f\"{name}_{index}.png\"\n",
        "            tem_mask_name = f\"{name}_{index}.png\"\n",
        "\n",
        "            image_path = os.path.join(save_aug_path, \"images\", tem_image_name)\n",
        "            mask_path = os.path.join(save_aug_path, \"masks\", tem_mask_name)\n",
        "\n",
        "            cv2.imwrite(image_path, i)\n",
        "            cv2.imwrite(mask_path, m)\n",
        "\n",
        "            index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEKf3wpm4ZjA"
      },
      "outputs": [],
      "source": [
        "smooth = 1e-15\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = Flatten()(y_true)\n",
        "    y_pred = Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2 * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbQphOyw4Zlo"
      },
      "outputs": [],
      "source": [
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intesection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intesection\n",
        "        x = (intesection + smooth) / (union + smooth)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs8k8yEU4ZoY"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kefxqSut4ZrB"
      },
      "outputs": [],
      "source": [
        "def load_data_for_train(path):\n",
        "    \"\"\"X == images , Y == masks\"\"\"\n",
        "    x = sorted(glob(os.path.join(path, \"images\", \"*.png\")))\n",
        "    y = sorted(glob(os.path.join(path, \"masks\", \"*.png\")))\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5y26mWk4Zt4"
      },
      "outputs": [],
      "source": [
        "def read_images(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    #x = cv2.resize(x , (W, H))\n",
        "    x = x / 255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHYUGTud5C2k"
      },
      "outputs": [],
      "source": [
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    #x = cv2.resize(x , (W, H))\n",
        "    #x = x / 255.0\n",
        "    x = x.astype(np.float32)\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "597AmgUx5C5U"
      },
      "outputs": [],
      "source": [
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_images(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ei4J37X5C8L"
      },
      "outputs": [],
      "source": [
        "def tf_dataset(X, y, batch=2):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(10)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZVpbBp_5C-8"
      },
      "outputs": [],
      "source": [
        "def squeeze_and_excite(inputs, ratio=8):\n",
        "    init = inputs\n",
        "    filters = init.shape[-1]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation=\"relu\", kernel_initializer=\"he_normal\", use_bias=False)(se)\n",
        "    se = Dense(filters, activation=\"sigmoid\", kernel_initializer=\"he_normal\", use_bias=False)(se)\n",
        "    x = init * se\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ET7Q5Da5DBs"
      },
      "outputs": [],
      "source": [
        "def ASPP(inputs):\n",
        "    #print(inputs.shape)\n",
        "    shape = inputs.shape\n",
        "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(inputs)\n",
        "    y1 = Conv2D(256, 1, padding=\"same\", use_bias=False)(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation(\"relu\")(y1)\n",
        "    y1 = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y1)\n",
        "\n",
        "    y2 = Conv2D(256, 1, padding=\"same\", use_bias=False)(inputs)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y2 = Activation(\"relu\")(y2)\n",
        "\n",
        "    y3 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=6)(inputs)\n",
        "    y3 = BatchNormalization()(y3)\n",
        "    y3 = Activation(\"relu\")(y3)\n",
        "\n",
        "    y4 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=12)(inputs)\n",
        "    y4 = BatchNormalization()(y4)\n",
        "    y4 = Activation(\"relu\")(y4)\n",
        "\n",
        "    y5 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=18)(inputs)\n",
        "    y5 = BatchNormalization()(y5)\n",
        "    y5 = Activation(\"relu\")(y5)\n",
        "\n",
        "    y = Concatenate()([y1, y2, y3, y4, y5])\n",
        "\n",
        "    y = Conv2D(256, 1, padding=\"same\", use_bias=False)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndD51NtZ5DEc"
      },
      "outputs": [],
      "source": [
        "def deeplabv3_plus(shape):\n",
        "    inputs = Input(shape)\n",
        "    encoder = ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
        "\n",
        "    image_features = encoder.get_layer(\"conv4_block6_out\").output\n",
        "    x_a = ASPP(image_features)\n",
        "    x_a = UpSampling2D((4, 4), interpolation=\"bilinear\")(x_a)\n",
        "\n",
        "    x_b = encoder.get_layer(\"conv2_block2_out\").output\n",
        "    x_b = Conv2D(filters=48, kernel_size=1, padding=\"same\", use_bias=False)(x_b)\n",
        "    x_b = BatchNormalization()(x_b)\n",
        "    x_b = Activation(\"relu\")(x_b)\n",
        "\n",
        "    x = Concatenate()([x_a, x_b])\n",
        "    x = squeeze_and_excite(x)\n",
        "\n",
        "    x = Conv2D(filters=256, kernel_size=3, padding=\"same\", use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(filters=256, kernel_size=3, padding=\"same\", use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = squeeze_and_excite(x)\n",
        "\n",
        "    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n",
        "    x = Conv2D(1, 1)(x)\n",
        "    x = Activation(\"sigmoid\")(x)\n",
        "\n",
        "    #print(x.shape)\n",
        "\n",
        "    model =Model(inputs, x)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDQhy83Y5DHy"
      },
      "outputs": [],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     model = deeplabv3_plus((512, 512, 3))\n",
        "#     model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js76CBf24Zw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2e8eb4-2b80-44f1-af69-c6e7f2616649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: \t 5111 - 5111\n",
            "test: \t 567 - 567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5111/5111 [11:04<00:00,  7.69it/s]\n",
            "100%|██████████| 567/567 [00:16<00:00, 34.27it/s]\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "\n",
        "    data_path = \"/content/people_segmentation/people_segmentation/\"\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = load_data(data_path)\n",
        "    print(f\"train: \\t {len(X_train)} - {len(y_train)}\")\n",
        "    print(f\"test: \\t {len(X_test)} - {len(y_test)}\")\n",
        "\n",
        "    create_dir(data_path + \"/new_data/train/images/\")\n",
        "    create_dir(data_path + \"/new_data/train/masks/\")\n",
        "    create_dir(data_path + \"/new_data/test/images/\")\n",
        "    create_dir(data_path + \"/new_data/test/masks/\")\n",
        "\n",
        "    augment_data(X_train, y_train, data_path + \"/new_data/train/\", augment=True)\n",
        "    augment_data(X_test, y_test, data_path + \"/new_data/test/\", augment=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkHEOJIY5Q83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823d310f-be05-4b87-fdb2-1b3259190da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30666 30666\n",
            "Training dataset : 30666 - 30666\n",
            "Validation dataset : 567 - 567\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"/content/people_segmentation/people_segmentation/new_data\"\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    create_dir(\"/content/drive/MyDrive/\" + \"/files\")\n",
        "\n",
        "    batch_size = 8\n",
        "    learning_rate = 1e-4\n",
        "    num_epochs = 5\n",
        "    model_path = os.path.join(\"/content/drive/MyDrive/\", \"files\", \"human_image_segmentation_18_epochs_deeplabv3.h5\") # Trained for 16 epochs\n",
        "    csv_path = os.path.join(\"/content/drive/MyDrive/\", \"files\", \"human_image_segmentation_18_epochs_deeplabv3.csv\")\n",
        "\n",
        "    #dataset_path = \"new_data\"\n",
        "\n",
        "    train_path = os.path.join(dataset_path, \"train\")\n",
        "    valid_path = os.path.join(dataset_path, \"test\")\n",
        "\n",
        "    X_train, y_train = load_data_for_train(train_path)\n",
        "    # print(\"Len training\", len(X_train), len(y_train))\n",
        "    # print(X_train[-1], y_train[-1])\n",
        "\n",
        "    # extra_element = [item for item in X_train if item not in y_train]\n",
        "\n",
        "    # if extra_element:\n",
        "    #     X_train.remove(extra_element[0])\n",
        "    #     print(f\"Extra element {extra_element[0]} found and deleted.\")\n",
        "    # else:\n",
        "    #     print(\"No extra element found.\")\n",
        "\n",
        "    print(len(X_train), len(y_train))  # Updated list1 without the extra element\n",
        "\n",
        "    X_train, y_val = shuffling(X_train, y_train)\n",
        "    # extra_element = [item for item in y_val if item not in X_train]\n",
        "\n",
        "    # if extra_element:\n",
        "    #     y_val.remove(extra_element[0])\n",
        "    #     print(f\"Extra element {extra_element[0]} found and deleted.\")\n",
        "    # else:\n",
        "    #     print(\"No extra element found.\")\n",
        "\n",
        "    # print(X_train)\n",
        "    X_val, y_val = load_data_for_train(valid_path)\n",
        "\n",
        "    print(f\"Training dataset : {len(X_train)} - {len(y_train)}\")\n",
        "    print(f\"Validation dataset : {len(X_val)} - {len(y_val)}\")\n",
        "\n",
        "    train_dataset = tf_dataset(X_train, y_train, batch=batch_size)\n",
        "    valid_dataset = tf_dataset(X_val, y_val, batch=batch_size)\n",
        "\n",
        "    #for x, y in train_dataset:\n",
        "        #print(x.shape, y.shape)\n",
        "        #break\n",
        "\n",
        "    #model = deeplabv3_plus((H, W, 3))\n",
        "    model = load_model(\"/content/drive/MyDrive/files/human_image_segmentation_16_epochs_deeplabv3.h5\", compile=False)\n",
        "    model.compile(\n",
        "        loss=dice_loss, optimizer=Adam(learning_rate=learning_rate),\n",
        "                 metrics=[dice_coef, iou, Recall(), Precision()]\n",
        "    )\n",
        "\n",
        "    #model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
        "        CSVLogger(csv_path),\n",
        "        #TensorBoard(),\n",
        "        EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=False)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJLlPW5m5Q_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159fd43c-0b98-4411-bf42-9190b9516d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "3834/3834 [==============================] - ETA: 0s - loss: 0.4326 - dice_coef: 0.5673 - iou: 0.4244 - recall: 0.8387 - precision: 0.4714\n",
            "Epoch 1: val_loss improved from inf to 0.43436, saving model to /content/drive/MyDrive/files/human_image_segmentation_16_epochs_deeplabv3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3834/3834 [==============================] - 3306s 845ms/step - loss: 0.4326 - dice_coef: 0.5673 - iou: 0.4244 - recall: 0.8387 - precision: 0.4714 - val_loss: 0.4344 - val_dice_coef: 0.5657 - val_iou: 0.4010 - val_recall: 0.8801 - val_precision: 0.4262 - lr: 1.0000e-04\n",
            "Epoch 2/5\n",
            "3834/3834 [==============================] - ETA: 0s - loss: 0.4284 - dice_coef: 0.5716 - iou: 0.4282 - recall: 0.8377 - precision: 0.4769\n",
            "Epoch 2: val_loss did not improve from 0.43436\n",
            "3834/3834 [==============================] - 3207s 836ms/step - loss: 0.4284 - dice_coef: 0.5716 - iou: 0.4282 - recall: 0.8377 - precision: 0.4769 - val_loss: 0.4472 - val_dice_coef: 0.5528 - val_iou: 0.3884 - val_recall: 0.8641 - val_precision: 0.4163 - lr: 1.0000e-04\n",
            "Epoch 3/5\n",
            "  70/3834 [..............................] - ETA: 52:27 - loss: 0.2950 - dice_coef: 0.7050 - iou: 0.5664 - recall: 0.8761 - precision: 0.6216"
          ]
        }
      ],
      "source": [
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CUVaUEG5RCf"
      },
      "outputs": [],
      "source": [
        "def save_results(image, mask, y_pred, save_image_path):\n",
        "    line = np.ones((H, 10, 3)) * 128\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    mask = np.concatenate([mask, mask, mask], axis=-1)\n",
        "    mask = mask * 255\n",
        "\n",
        "    y_pred = np.expand_dims(y_pred, axis=-1)\n",
        "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1)\n",
        "    #y_pred = y_pred * 255\n",
        "\n",
        "    mask_image = image * y_pred\n",
        "    y_pred = y_pred * 255\n",
        "\n",
        "    concatinate_images = np.concatenate([image, line, mask, line, y_pred, line, mask_image], axis=1)\n",
        "\n",
        "    cv2.imwrite(save_image_path, concatinate_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZUlA0yR5RFP"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/people_segmentation/people_segmentation/new_data\"\n",
        "human_image_seg_model_500_epochs = '/content/drive/MyDrive/human_image_segmentation_10_epochs_deeplabv3.h5'\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    create_dir(dataset_path + \"/results\")\n",
        "\n",
        "    with CustomObjectScope({\"iou\":iou, \"dice_coef\":dice_coef, \"dice_loss\":dice_loss}):\n",
        "        human_image_seg_model = load_model(human_image_seg_model_500_epochs, compile=False)\n",
        "        #retina_blood_model.summary()\n",
        "\n",
        "    #dataset_path = \"new_data\"\n",
        "\n",
        "    valid_path = os.path.join(dataset_path, \"test\")\n",
        "\n",
        "    X_test, y_test = load_data_for_train(valid_path)\n",
        "    print(f\"Test: {len(X_test)} - {len(y_test)}\")\n",
        "    #print(X_test, y_test)\n",
        "\n",
        "    score = []\n",
        "    for x, y in tqdm(zip(X_test, y_test), total=len(X_test)):\n",
        "        #print(x, y)\n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "        print(name)\n",
        "\n",
        "        image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        x = image / 255.0\n",
        "        #print(x.shape)\n",
        "\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        #print(x.shape)\n",
        "\n",
        "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        y_pred = human_image_seg_model.predict(x)[0]\n",
        "        y_pred = np.squeeze(y_pred, axis=-1)\n",
        "        y_pred = y_pred > 0.5\n",
        "        #print(y_pred.shape)\n",
        "        y_pred = y_pred.astype(np.int32)\n",
        "        #y_pred = np.squeeze(y_pred, axis=-1)\n",
        "        #print(y_pred.shape)\n",
        "\n",
        "        save_image_path = f\"/content/drive/MyDrive/people_segmentation/people_segmentation/new_data/results/{name}.png\"\n",
        "\n",
        "        save_results(image, mask, y_pred, save_image_path)\n",
        "\n",
        "        mask = mask.flatten()\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "        accuracy_value = accuracy_score(mask, y_pred)\n",
        "        f1_value = f1_score(mask, y_pred, labels=[0, 1], average=\"binary\")\n",
        "        jac_value = jaccard_score(mask, y_pred, labels=[0, 1], average=\"binary\")\n",
        "        recall_value = recall_score(mask, y_pred, labels=[0, 1], average=\"binary\")\n",
        "        precision_value = precision_score(mask, y_pred, labels=[0, 1], average=\"binary\")\n",
        "\n",
        "        score.append([name, accuracy_value, f1_value, jac_value, recall_value, precision_value])\n",
        "\n",
        "    score = [s[1:] for s in score]\n",
        "    score = np.mean(score, axis=0)\n",
        "\n",
        "    print(f\"Accuracy: {score[0]:0.5f}\")\n",
        "    print(f\"F1: {score[1]:0.5f}\")\n",
        "    print(f\"Jaccard: {score[2]:0.5f}\")\n",
        "    print(f\"Recall: {score[3]:0.5f}\")\n",
        "    print(f\"Precision: {score[4]:0.5f}\")\n",
        "\n",
        "\n",
        "    #df = pd.DataFrame(score, columns=[\"Image\", \"Accuracy\", \"F1_score\", \"Jaccard\", \"Recall\", \"Precision\"])\n",
        "    #df.to_csv(\"files\\\\score.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNyRLUZW5RH3"
      },
      "outputs": [],
      "source": [
        "\"\"\"predict\"\"\"\n",
        "human_image_seg_model = \"human_image_seg_model_500_epochs\"\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    create_dir(\"test_image/mask\")\n",
        "\n",
        "    with CustomObjectScope({\"iou\":iou, \"dice_coef\":dice_coef, \"dice_loss\":dice_loss}):\n",
        "        human_image_seg_model = load_model(retina_blood_model_path)\n",
        "\n",
        "    x_data = glob(\"test_image/iamge/*\")\n",
        "\n",
        "    for path in tqdm(x_data, total=len(x_data)):\n",
        "        name = path.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "        h, w, _ = image.shape\n",
        "        x = cv2.resize(image, (W, H))\n",
        "        x = x / 255.0\n",
        "        x = x.astype(np.float32)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "\n",
        "        y = human_image_seg_model.predice(x)[0]\n",
        "        y = cv2.resize(y, (w, h))\n",
        "        y = np.expand_dims(y, axis=-1)\n",
        "\n",
        "        masked_image = image * y\n",
        "        line = np.ones((h, 10, 3)) * 128\n",
        "        cat_image = np.concatenate([image, line, masked_image], axis=1)\n",
        "        cv2.imwrite(f\"test_image/mask/{name}.png\", cat_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHzvkq2i5RKn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIJKqleW5RNO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXD5FN-25RP3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_stDIrJ95RSY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ornxdMn65RU_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G_Q-2pY5RXr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}