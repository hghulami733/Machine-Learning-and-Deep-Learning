{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a091bb8-6681-4020-8cdd-f09625e82f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model \n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d5395d-66b7-49c5-9965-72e096e4d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 512\n",
    "W = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e059e99e-e457-4a9b-84bf-5c968b2c699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b2c9b1f-4ea8-4c90-99d1-3b50162d0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes=196):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    backbone = MobileNetV2(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=inputs,\n",
    "        alpha=1.0\n",
    "    )\n",
    "\n",
    "    #backbone.summary()\n",
    "\n",
    "    X = backbone.output\n",
    "    #y = backbone.get_layer(\"block_13_expand_relu\").output\n",
    "    X = Conv2D(256, kernel_size=1, padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(4, activation=\"sigmoid\")(X)\n",
    "\n",
    "    model = Model(inputs, X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "942aca12-eb5c-4e8e-9503-ce7996686bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 512, 512, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)              (None, 256, 256, 32)         864       ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalizati  (None, 256, 256, 32)         128       ['Conv1[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)           (None, 256, 256, 32)         0         ['bn_Conv1[0][0]']            \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (D  (None, 256, 256, 32)         288       ['Conv1_relu[0][0]']          \n",
      " epthwiseConv2D)                                                                                  \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN  (None, 256, 256, 32)         128       ['expanded_conv_depthwise[0][0\n",
      "  (BatchNormalization)                                              ]']                           \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_re  (None, 256, 256, 32)         0         ['expanded_conv_depthwise_BN[0\n",
      " lu (ReLU)                                                          ][0]']                        \n",
      "                                                                                                  \n",
      " expanded_conv_project (Con  (None, 256, 256, 16)         512       ['expanded_conv_depthwise_relu\n",
      " v2D)                                                               [0][0]']                      \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (  (None, 256, 256, 16)         64        ['expanded_conv_project[0][0]'\n",
      " BatchNormalization)                                                ]                             \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)     (None, 256, 256, 96)         1536      ['expanded_conv_project_BN[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNo  (None, 256, 256, 96)         384       ['block_1_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)  (None, 256, 256, 96)         0         ['block_1_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D  (None, 257, 257, 96)         0         ['block_1_expand_relu[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_1_depthwise (Depthwi  (None, 128, 128, 96)         864       ['block_1_pad[0][0]']         \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (Batc  (None, 128, 128, 96)         384       ['block_1_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (Re  (None, 128, 128, 96)         0         ['block_1_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)    (None, 128, 128, 24)         2304      ['block_1_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchN  (None, 128, 128, 24)         96        ['block_1_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)     (None, 128, 128, 144)        3456      ['block_1_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNo  (None, 128, 128, 144)        576       ['block_2_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)  (None, 128, 128, 144)        0         ['block_2_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_depthwise (Depthwi  (None, 128, 128, 144)        1296      ['block_2_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (Batc  (None, 128, 128, 144)        576       ['block_2_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (Re  (None, 128, 128, 144)        0         ['block_2_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)    (None, 128, 128, 24)         3456      ['block_2_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchN  (None, 128, 128, 24)         96        ['block_2_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_2_add (Add)           (None, 128, 128, 24)         0         ['block_1_project_BN[0][0]',  \n",
      "                                                                     'block_2_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)     (None, 128, 128, 144)        3456      ['block_2_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNo  (None, 128, 128, 144)        576       ['block_3_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)  (None, 128, 128, 144)        0         ['block_3_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D  (None, 129, 129, 144)        0         ['block_3_expand_relu[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_3_depthwise (Depthwi  (None, 64, 64, 144)          1296      ['block_3_pad[0][0]']         \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (Batc  (None, 64, 64, 144)          576       ['block_3_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (Re  (None, 64, 64, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)    (None, 64, 64, 32)           4608      ['block_3_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchN  (None, 64, 64, 32)           128       ['block_3_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)     (None, 64, 64, 192)          6144      ['block_3_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNo  (None, 64, 64, 192)          768       ['block_4_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)  (None, 64, 64, 192)          0         ['block_4_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_depthwise (Depthwi  (None, 64, 64, 192)          1728      ['block_4_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (Batc  (None, 64, 64, 192)          768       ['block_4_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (Re  (None, 64, 64, 192)          0         ['block_4_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)    (None, 64, 64, 32)           6144      ['block_4_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchN  (None, 64, 64, 32)           128       ['block_4_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_4_add (Add)           (None, 64, 64, 32)           0         ['block_3_project_BN[0][0]',  \n",
      "                                                                     'block_4_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)     (None, 64, 64, 192)          6144      ['block_4_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNo  (None, 64, 64, 192)          768       ['block_5_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)  (None, 64, 64, 192)          0         ['block_5_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_depthwise (Depthwi  (None, 64, 64, 192)          1728      ['block_5_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (Batc  (None, 64, 64, 192)          768       ['block_5_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (Re  (None, 64, 64, 192)          0         ['block_5_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)    (None, 64, 64, 32)           6144      ['block_5_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchN  (None, 64, 64, 32)           128       ['block_5_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_5_add (Add)           (None, 64, 64, 32)           0         ['block_4_add[0][0]',         \n",
      "                                                                     'block_5_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)     (None, 64, 64, 192)          6144      ['block_5_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNo  (None, 64, 64, 192)          768       ['block_6_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)  (None, 64, 64, 192)          0         ['block_6_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D  (None, 65, 65, 192)          0         ['block_6_expand_relu[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_6_depthwise (Depthwi  (None, 32, 32, 192)          1728      ['block_6_pad[0][0]']         \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (Batc  (None, 32, 32, 192)          768       ['block_6_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (Re  (None, 32, 32, 192)          0         ['block_6_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)    (None, 32, 32, 64)           12288     ['block_6_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchN  (None, 32, 32, 64)           256       ['block_6_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)     (None, 32, 32, 384)          24576     ['block_6_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNo  (None, 32, 32, 384)          1536      ['block_7_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)  (None, 32, 32, 384)          0         ['block_7_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_depthwise (Depthwi  (None, 32, 32, 384)          3456      ['block_7_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (Batc  (None, 32, 32, 384)          1536      ['block_7_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (Re  (None, 32, 32, 384)          0         ['block_7_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)    (None, 32, 32, 64)           24576     ['block_7_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchN  (None, 32, 32, 64)           256       ['block_7_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_7_add (Add)           (None, 32, 32, 64)           0         ['block_6_project_BN[0][0]',  \n",
      "                                                                     'block_7_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)     (None, 32, 32, 384)          24576     ['block_7_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNo  (None, 32, 32, 384)          1536      ['block_8_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)  (None, 32, 32, 384)          0         ['block_8_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_depthwise (Depthwi  (None, 32, 32, 384)          3456      ['block_8_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (Batc  (None, 32, 32, 384)          1536      ['block_8_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (Re  (None, 32, 32, 384)          0         ['block_8_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)    (None, 32, 32, 64)           24576     ['block_8_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchN  (None, 32, 32, 64)           256       ['block_8_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_8_add (Add)           (None, 32, 32, 64)           0         ['block_7_add[0][0]',         \n",
      "                                                                     'block_8_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)     (None, 32, 32, 384)          24576     ['block_8_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNo  (None, 32, 32, 384)          1536      ['block_9_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)  (None, 32, 32, 384)          0         ['block_9_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_depthwise (Depthwi  (None, 32, 32, 384)          3456      ['block_9_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (Batc  (None, 32, 32, 384)          1536      ['block_9_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (Re  (None, 32, 32, 384)          0         ['block_9_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)    (None, 32, 32, 64)           24576     ['block_9_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchN  (None, 32, 32, 64)           256       ['block_9_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_9_add (Add)           (None, 32, 32, 64)           0         ['block_8_add[0][0]',         \n",
      "                                                                     'block_9_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)    (None, 32, 32, 384)          24576     ['block_9_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchN  (None, 32, 32, 384)          1536      ['block_10_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU  (None, 32, 32, 384)          0         ['block_10_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_10_depthwise (Depthw  (None, 32, 32, 384)          3456      ['block_10_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (Bat  (None, 32, 32, 384)          1536      ['block_10_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (R  (None, 32, 32, 384)          0         ['block_10_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)   (None, 32, 32, 96)           36864     ['block_10_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_10_project_BN (Batch  (None, 32, 32, 96)           384       ['block_10_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)    (None, 32, 32, 576)          55296     ['block_10_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchN  (None, 32, 32, 576)          2304      ['block_11_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU  (None, 32, 32, 576)          0         ['block_11_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_11_depthwise (Depthw  (None, 32, 32, 576)          5184      ['block_11_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (Bat  (None, 32, 32, 576)          2304      ['block_11_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (R  (None, 32, 32, 576)          0         ['block_11_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)   (None, 32, 32, 96)           55296     ['block_11_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_11_project_BN (Batch  (None, 32, 32, 96)           384       ['block_11_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_11_add (Add)          (None, 32, 32, 96)           0         ['block_10_project_BN[0][0]', \n",
      "                                                                     'block_11_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)    (None, 32, 32, 576)          55296     ['block_11_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchN  (None, 32, 32, 576)          2304      ['block_12_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU  (None, 32, 32, 576)          0         ['block_12_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_12_depthwise (Depthw  (None, 32, 32, 576)          5184      ['block_12_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (Bat  (None, 32, 32, 576)          2304      ['block_12_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (R  (None, 32, 32, 576)          0         ['block_12_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)   (None, 32, 32, 96)           55296     ['block_12_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_12_project_BN (Batch  (None, 32, 32, 96)           384       ['block_12_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_12_add (Add)          (None, 32, 32, 96)           0         ['block_11_add[0][0]',        \n",
      "                                                                     'block_12_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)    (None, 32, 32, 576)          55296     ['block_12_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchN  (None, 32, 32, 576)          2304      ['block_13_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU  (None, 32, 32, 576)          0         ['block_13_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2  (None, 33, 33, 576)          0         ['block_13_expand_relu[0][0]']\n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block_13_depthwise (Depthw  (None, 16, 16, 576)          5184      ['block_13_pad[0][0]']        \n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (Bat  (None, 16, 16, 576)          2304      ['block_13_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (R  (None, 16, 16, 576)          0         ['block_13_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)   (None, 16, 16, 160)          92160     ['block_13_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_13_project_BN (Batch  (None, 16, 16, 160)          640       ['block_13_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)    (None, 16, 16, 960)          153600    ['block_13_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchN  (None, 16, 16, 960)          3840      ['block_14_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU  (None, 16, 16, 960)          0         ['block_14_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_14_depthwise (Depthw  (None, 16, 16, 960)          8640      ['block_14_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (Bat  (None, 16, 16, 960)          3840      ['block_14_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (R  (None, 16, 16, 960)          0         ['block_14_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)   (None, 16, 16, 160)          153600    ['block_14_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_14_project_BN (Batch  (None, 16, 16, 160)          640       ['block_14_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_14_add (Add)          (None, 16, 16, 160)          0         ['block_13_project_BN[0][0]', \n",
      "                                                                     'block_14_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)    (None, 16, 16, 960)          153600    ['block_14_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchN  (None, 16, 16, 960)          3840      ['block_15_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU  (None, 16, 16, 960)          0         ['block_15_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_15_depthwise (Depthw  (None, 16, 16, 960)          8640      ['block_15_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (Bat  (None, 16, 16, 960)          3840      ['block_15_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (R  (None, 16, 16, 960)          0         ['block_15_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)   (None, 16, 16, 160)          153600    ['block_15_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_15_project_BN (Batch  (None, 16, 16, 160)          640       ['block_15_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_15_add (Add)          (None, 16, 16, 160)          0         ['block_14_add[0][0]',        \n",
      "                                                                     'block_15_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)    (None, 16, 16, 960)          153600    ['block_15_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchN  (None, 16, 16, 960)          3840      ['block_16_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU  (None, 16, 16, 960)          0         ['block_16_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_16_depthwise (Depthw  (None, 16, 16, 960)          8640      ['block_16_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (Bat  (None, 16, 16, 960)          3840      ['block_16_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (R  (None, 16, 16, 960)          0         ['block_16_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)   (None, 16, 16, 320)          307200    ['block_16_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_16_project_BN (Batch  (None, 16, 16, 320)          1280      ['block_16_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)             (None, 16, 16, 1280)         409600    ['block_16_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalizat  (None, 16, 16, 1280)         5120      ['Conv_1[0][0]']              \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " out_relu (ReLU)             (None, 16, 16, 1280)         0         ['Conv_1_bn[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 256)          327936    ['out_relu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 16, 16, 256)          1024      ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 16, 16, 256)          0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling2d_5  (None, 256)                  0         ['activation_5[0][0]']        \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 256)                  0         ['global_average_pooling2d_5[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 4)                    1028      ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2587972 (9.87 MB)\n",
      "Trainable params: 2553348 (9.74 MB)\n",
      "Non-trainable params: 34624 (135.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    input_shape = (512, 512, 3)\n",
    "\n",
    "    model = build_model(input_shape)\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7fc314ea-ea06-4274-8f11-dfe5147c1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, split=0.1):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "\n",
    "    df = pd.read_csv(os.path.join(path, \"bbox.csv\"))\n",
    "    for index, row in df.iterrows():\n",
    "        #print(index, row)\n",
    "        name = row[\"name\"]\n",
    "        X1 = int(row[\"x1\"])\n",
    "        y1 = int(row[\"y1\"])\n",
    "        X2 = int(row[\"x2\"])\n",
    "        y2 = int(row[\"y2\"])\n",
    "\n",
    "        image = os.path.join(path, \"images\", name)\n",
    "        bbox = [X1, y1, X2, y2]\n",
    "\n",
    "        images.append(image)\n",
    "        bboxes.append(bbox)\n",
    "\n",
    "    #print(len(images), len(bboxes))\n",
    "    split_size = int(len(images) * split)\n",
    "\n",
    "    X_train, X_val = train_test_split(images, test_size=split_size, random_state=42)\n",
    "    y_train, y_val = train_test_split(bboxes, test_size=split_size, random_state=42)\n",
    "\n",
    "    X_train, X_test = train_test_split(X_train, test_size=split_size, random_state=42)\n",
    "    y_train, y_test = train_test_split(y_train, test_size=split_size, random_state=42)\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77d488df-ae68-4dae-984d-b1c20efe7ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_bbox(path, bbox):\n",
    "    path = path.decode()\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "    image = cv2.resize(image, (W, H))\n",
    "    image = (image - 127.5) / 127.5\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    X1, y1, X2, y2 = bbox\n",
    "    norm_x1 = float(X1 / w)\n",
    "    norm_y1 = float(y1 / h)\n",
    "    norm_x2 = float(X2 / w)\n",
    "    norm_y2 = float(y2 / h)\n",
    "\n",
    "    norm_bbox = np.array([norm_x1, norm_y1, norm_x2, norm_y2], dtype=np.float32)\n",
    "    return image, norm_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffae5bba-06f3-4595-84de-952ec0d2cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(x, y):\n",
    "    x, y = tf.numpy_function(read_image_bbox, [x, y], [tf.float32, tf.float32])\n",
    "    x.set_shape([W, H, 3])\n",
    "    y.set_shape([4])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "192485e1-bfbe-4896-81bc-ac3ea388297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dataset(images, bboxes, batch=8):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, bboxes))\n",
    "    ds = ds.map(parse).batch(batch).prefetch(10)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4c5f80d-5a61-4241-9eef-0c565583d936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 679, 679\n",
      "Val: 84, 84\n",
      "Test: 84, 84\n",
      "WARNING:tensorflow:From C:\\Users\\hamid\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "WARNING:tensorflow:From C:\\Users\\hamid\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = r\"E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)\"\n",
    "dataset_path = r\"E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)\\images\"\n",
    "if __name__==\"__main__\":\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    create_dir(data_path + \"\\\\files\")\n",
    "\n",
    "    data = os.path.join(data_path, \"\\\\files\")\n",
    "\n",
    "    batch_size = 8\n",
    "    lr = 1e-4\n",
    "    num_epochs = 100\n",
    "    \n",
    "    model_path = os.path.join(data_path + \"files\" + \"human_detection_100_epochs.h5\")\n",
    "    csv_path = os.path.join(data_path + \"files\" + \"log.csv\")\n",
    "\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_data(data_path)\n",
    "    print(f\"Train: {len(X_train)}, {len(y_train)}\")\n",
    "    print(f\"Val: {len(X_val)}, {len(y_val)}\")\n",
    "    print(f\"Test: {len(X_test)}, {len(y_test)}\")\n",
    "\n",
    "    train_dataset = tf_dataset(X_train, y_train, batch=batch_size)\n",
    "    val_dataset = tf_dataset(X_val, y_val, batch=batch_size)\n",
    "\n",
    "    #for x, y in train_dataset:\n",
    "        #print(x.shape, y.shape)\n",
    "        #idx = 3\n",
    "        #image = (x[idx].numpy() + 1) * 127.5\n",
    "        #X1 = int(y[idx][0] * image.shape[1])\n",
    "        #y1 = int(y[idx][1] * image.shape[0])\n",
    "        #X2 = int(y[idx][2] * image.shape[1]) \n",
    "        #y2 = int(y[idx][3] * image.shape[0])\n",
    "\n",
    "        #image = cv2.rectangle(image, (X1, y1), (X2, y2), (0, 255, 0), 5)\n",
    "        #cv2.imwrite(data_path + \"\\\\1.png\", image)\n",
    "        \n",
    "        #break\n",
    "\n",
    "    model = build_model((H, W, 3))\n",
    "    model.compile(\n",
    "        loss = \"binary_crossentropy\",\n",
    "        optimizer=Adam(learning_rate=lr)\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
    "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_delta=1),\n",
    "        CSVLogger(csv_path, append=True),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=False)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64082ac9-b6a8-4e39-94d3-6a31e9c4bf5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.6685\n",
      "Epoch 1: val_loss improved from inf to 0.58329, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamid\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 219s 2s/step - loss: 0.6685 - val_loss: 0.5833 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4902\n",
      "Epoch 2: val_loss improved from 0.58329 to 0.47854, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 140s 2s/step - loss: 0.4902 - val_loss: 0.4785 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4620\n",
      "Epoch 3: val_loss improved from 0.47854 to 0.46504, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4620 - val_loss: 0.4650 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4513\n",
      "Epoch 4: val_loss improved from 0.46504 to 0.46328, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4513 - val_loss: 0.4633 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4408\n",
      "Epoch 5: val_loss improved from 0.46328 to 0.46109, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4408 - val_loss: 0.4611 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4349\n",
      "Epoch 6: val_loss improved from 0.46109 to 0.45618, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 140s 2s/step - loss: 0.4349 - val_loss: 0.4562 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4297\n",
      "Epoch 7: val_loss improved from 0.45618 to 0.45523, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4297 - val_loss: 0.4552 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4295\n",
      "Epoch 8: val_loss improved from 0.45523 to 0.45398, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 140s 2s/step - loss: 0.4295 - val_loss: 0.4540 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4285\n",
      "Epoch 9: val_loss did not improve from 0.45398\n",
      "85/85 [==============================] - 139s 2s/step - loss: 0.4285 - val_loss: 0.4548 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4286\n",
      "Epoch 10: val_loss did not improve from 0.45398\n",
      "85/85 [==============================] - 139s 2s/step - loss: 0.4286 - val_loss: 0.4546 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4273\n",
      "Epoch 11: val_loss improved from 0.45398 to 0.45389, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4273 - val_loss: 0.4539 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4275\n",
      "Epoch 12: val_loss improved from 0.45389 to 0.45298, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 140s 2s/step - loss: 0.4275 - val_loss: 0.4530 - lr: 1.0000e-06\n",
      "Epoch 13/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4270\n",
      "Epoch 13: val_loss improved from 0.45298 to 0.45224, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 140s 2s/step - loss: 0.4270 - val_loss: 0.4522 - lr: 1.0000e-06\n",
      "Epoch 14/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4259\n",
      "Epoch 14: val_loss improved from 0.45224 to 0.45174, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 140s 2s/step - loss: 0.4259 - val_loss: 0.4517 - lr: 1.0000e-06\n",
      "Epoch 15/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4261\n",
      "Epoch 15: val_loss improved from 0.45174 to 0.45128, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4261 - val_loss: 0.4513 - lr: 1.0000e-06\n",
      "Epoch 16/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4266\n",
      "Epoch 16: val_loss improved from 0.45128 to 0.45089, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 139s 2s/step - loss: 0.4266 - val_loss: 0.4509 - lr: 1.0000e-06\n",
      "Epoch 17/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4254\n",
      "Epoch 17: val_loss improved from 0.45089 to 0.45054, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 139s 2s/step - loss: 0.4254 - val_loss: 0.4505 - lr: 1.0000e-07\n",
      "Epoch 18/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4254\n",
      "Epoch 18: val_loss improved from 0.45054 to 0.45019, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4254 - val_loss: 0.4502 - lr: 1.0000e-07\n",
      "Epoch 19/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4257\n",
      "Epoch 19: val_loss improved from 0.45019 to 0.44986, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 142s 2s/step - loss: 0.4257 - val_loss: 0.4499 - lr: 1.0000e-07\n",
      "Epoch 20/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4267\n",
      "Epoch 20: val_loss improved from 0.44986 to 0.44960, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4267 - val_loss: 0.4496 - lr: 1.0000e-07\n",
      "Epoch 21/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4252\n",
      "Epoch 21: val_loss improved from 0.44960 to 0.44938, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4252 - val_loss: 0.4494 - lr: 1.0000e-07\n",
      "Epoch 22/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4262\n",
      "Epoch 22: val_loss improved from 0.44938 to 0.44919, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4262 - val_loss: 0.4492 - lr: 1.0000e-08\n",
      "Epoch 23/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4250\n",
      "Epoch 23: val_loss improved from 0.44919 to 0.44903, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 140s 2s/step - loss: 0.4250 - val_loss: 0.4490 - lr: 1.0000e-08\n",
      "Epoch 24/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4265\n",
      "Epoch 24: val_loss improved from 0.44903 to 0.44890, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4265 - val_loss: 0.4489 - lr: 1.0000e-08\n",
      "Epoch 25/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4259\n",
      "Epoch 25: val_loss improved from 0.44890 to 0.44879, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4259 - val_loss: 0.4488 - lr: 1.0000e-08\n",
      "Epoch 26/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4266\n",
      "Epoch 26: val_loss improved from 0.44879 to 0.44869, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 141s 2s/step - loss: 0.4266 - val_loss: 0.4487 - lr: 1.0000e-08\n",
      "Epoch 27/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4262\n",
      "Epoch 27: val_loss improved from 0.44869 to 0.44860, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 139s 2s/step - loss: 0.4262 - val_loss: 0.4486 - lr: 1.0000e-09\n",
      "Epoch 28/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4270\n",
      "Epoch 28: val_loss improved from 0.44860 to 0.44851, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4270 - val_loss: 0.4485 - lr: 1.0000e-09\n",
      "Epoch 29/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4262\n",
      "Epoch 29: val_loss improved from 0.44851 to 0.44843, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4262 - val_loss: 0.4484 - lr: 1.0000e-09\n",
      "Epoch 30/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4253\n",
      "Epoch 30: val_loss improved from 0.44843 to 0.44835, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 138s 2s/step - loss: 0.4253 - val_loss: 0.4484 - lr: 1.0000e-09\n",
      "Epoch 31/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4252\n",
      "Epoch 31: val_loss improved from 0.44835 to 0.44829, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4252 - val_loss: 0.4483 - lr: 1.0000e-09\n",
      "Epoch 32/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4260\n",
      "Epoch 32: val_loss improved from 0.44829 to 0.44823, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4260 - val_loss: 0.4482 - lr: 1.0000e-10\n",
      "Epoch 33/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4259\n",
      "Epoch 33: val_loss improved from 0.44823 to 0.44819, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4259 - val_loss: 0.4482 - lr: 1.0000e-10\n",
      "Epoch 34/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4257\n",
      "Epoch 34: val_loss improved from 0.44819 to 0.44815, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4257 - val_loss: 0.4481 - lr: 1.0000e-10\n",
      "Epoch 35/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4262\n",
      "Epoch 35: val_loss improved from 0.44815 to 0.44811, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4262 - val_loss: 0.4481 - lr: 1.0000e-10\n",
      "Epoch 36/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4270\n",
      "Epoch 36: val_loss improved from 0.44811 to 0.44808, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 138s 2s/step - loss: 0.4270 - val_loss: 0.4481 - lr: 1.0000e-10\n",
      "Epoch 37/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4258\n",
      "Epoch 37: val_loss improved from 0.44808 to 0.44805, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4258 - val_loss: 0.4481 - lr: 1.0000e-11\n",
      "Epoch 38/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4249\n",
      "Epoch 38: val_loss improved from 0.44805 to 0.44803, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4249 - val_loss: 0.4480 - lr: 1.0000e-11\n",
      "Epoch 39/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4265\n",
      "Epoch 39: val_loss improved from 0.44803 to 0.44801, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4265 - val_loss: 0.4480 - lr: 1.0000e-11\n",
      "Epoch 40/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4252\n",
      "Epoch 40: val_loss improved from 0.44801 to 0.44799, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4252 - val_loss: 0.4480 - lr: 1.0000e-11\n",
      "Epoch 41/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4262\n",
      "Epoch 41: val_loss improved from 0.44799 to 0.44797, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4262 - val_loss: 0.4480 - lr: 1.0000e-11\n",
      "Epoch 42/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4252\n",
      "Epoch 42: val_loss improved from 0.44797 to 0.44796, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4252 - val_loss: 0.4480 - lr: 1.0000e-12\n",
      "Epoch 43/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4263\n",
      "Epoch 43: val_loss improved from 0.44796 to 0.44795, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4263 - val_loss: 0.4479 - lr: 1.0000e-12\n",
      "Epoch 44/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4263\n",
      "Epoch 44: val_loss improved from 0.44795 to 0.44794, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4263 - val_loss: 0.4479 - lr: 1.0000e-12\n",
      "Epoch 45/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4263\n",
      "Epoch 45: val_loss improved from 0.44794 to 0.44793, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4263 - val_loss: 0.4479 - lr: 1.0000e-12\n",
      "Epoch 46/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4270\n",
      "Epoch 46: val_loss improved from 0.44793 to 0.44792, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4270 - val_loss: 0.4479 - lr: 1.0000e-12\n",
      "Epoch 47/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4269\n",
      "Epoch 47: val_loss improved from 0.44792 to 0.44792, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4269 - val_loss: 0.4479 - lr: 1.0000e-13\n",
      "Epoch 48/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4268\n",
      "Epoch 48: val_loss improved from 0.44792 to 0.44791, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4268 - val_loss: 0.4479 - lr: 1.0000e-13\n",
      "Epoch 49/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4258\n",
      "Epoch 49: val_loss improved from 0.44791 to 0.44791, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4258 - val_loss: 0.4479 - lr: 1.0000e-13\n",
      "Epoch 50/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4260\n",
      "Epoch 50: val_loss improved from 0.44791 to 0.44790, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4260 - val_loss: 0.4479 - lr: 1.0000e-13\n",
      "Epoch 51/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4255\n",
      "Epoch 51: val_loss improved from 0.44790 to 0.44790, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4255 - val_loss: 0.4479 - lr: 1.0000e-13\n",
      "Epoch 52/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4259\n",
      "Epoch 52: val_loss improved from 0.44790 to 0.44790, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4259 - val_loss: 0.4479 - lr: 1.0000e-14\n",
      "Epoch 53/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4267\n",
      "Epoch 53: val_loss improved from 0.44790 to 0.44790, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4267 - val_loss: 0.4479 - lr: 1.0000e-14\n",
      "Epoch 54/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4264\n",
      "Epoch 54: val_loss improved from 0.44790 to 0.44790, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4264 - val_loss: 0.4479 - lr: 1.0000e-14\n",
      "Epoch 55/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4269\n",
      "Epoch 55: val_loss improved from 0.44790 to 0.44790, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4269 - val_loss: 0.4479 - lr: 1.0000e-14\n",
      "Epoch 56/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4273\n",
      "Epoch 56: val_loss improved from 0.44790 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4273 - val_loss: 0.4479 - lr: 1.0000e-14\n",
      "Epoch 57/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4271\n",
      "Epoch 57: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4271 - val_loss: 0.4479 - lr: 1.0000e-15\n",
      "Epoch 58/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4262\n",
      "Epoch 58: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4262 - val_loss: 0.4479 - lr: 1.0000e-15\n",
      "Epoch 59/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4263\n",
      "Epoch 59: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4263 - val_loss: 0.4479 - lr: 1.0000e-15\n",
      "Epoch 60/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4261\n",
      "Epoch 60: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4261 - val_loss: 0.4479 - lr: 1.0000e-15\n",
      "Epoch 61/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4268\n",
      "Epoch 61: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4268 - val_loss: 0.4479 - lr: 1.0000e-15\n",
      "Epoch 62/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4263\n",
      "Epoch 62: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4263 - val_loss: 0.4479 - lr: 1.0000e-16\n",
      "Epoch 63/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4260\n",
      "Epoch 63: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4260 - val_loss: 0.4479 - lr: 1.0000e-16\n",
      "Epoch 64/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4272\n",
      "Epoch 64: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4272 - val_loss: 0.4479 - lr: 1.0000e-16\n",
      "Epoch 65/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4261\n",
      "Epoch 65: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4261 - val_loss: 0.4479 - lr: 1.0000e-16\n",
      "Epoch 66/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4259\n",
      "Epoch 66: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4259 - val_loss: 0.4479 - lr: 1.0000e-16\n",
      "Epoch 67/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4251\n",
      "Epoch 67: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4251 - val_loss: 0.4479 - lr: 1.0000e-17\n",
      "Epoch 68/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4255\n",
      "Epoch 68: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4255 - val_loss: 0.4479 - lr: 1.0000e-17\n",
      "Epoch 69/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4268\n",
      "Epoch 69: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4268 - val_loss: 0.4479 - lr: 1.0000e-17\n",
      "Epoch 70/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4261\n",
      "Epoch 70: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4261 - val_loss: 0.4479 - lr: 1.0000e-17\n",
      "Epoch 71/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4266\n",
      "Epoch 71: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4266 - val_loss: 0.4479 - lr: 1.0000e-17\n",
      "Epoch 72/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4254\n",
      "Epoch 72: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4254 - val_loss: 0.4479 - lr: 1.0000e-18\n",
      "Epoch 73/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4261\n",
      "Epoch 73: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4261 - val_loss: 0.4479 - lr: 1.0000e-18\n",
      "Epoch 74/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4264\n",
      "Epoch 74: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4264 - val_loss: 0.4479 - lr: 1.0000e-18\n",
      "Epoch 75/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4265\n",
      "Epoch 75: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4265 - val_loss: 0.4479 - lr: 1.0000e-18\n",
      "Epoch 76/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4258\n",
      "Epoch 76: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4258 - val_loss: 0.4479 - lr: 1.0000e-18\n",
      "Epoch 77/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4259\n",
      "Epoch 77: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4259 - val_loss: 0.4479 - lr: 1.0000e-19\n",
      "Epoch 78/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4265\n",
      "Epoch 78: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4265 - val_loss: 0.4479 - lr: 1.0000e-19\n",
      "Epoch 79/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4276\n",
      "Epoch 79: val_loss did not improve from 0.44789\n",
      "85/85 [==============================] - 139s 2s/step - loss: 0.4276 - val_loss: 0.4479 - lr: 1.0000e-19\n",
      "Epoch 80/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4261\n",
      "Epoch 80: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4261 - val_loss: 0.4479 - lr: 1.0000e-19\n",
      "Epoch 81/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4249\n",
      "Epoch 81: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4249 - val_loss: 0.4479 - lr: 1.0000e-19\n",
      "Epoch 82/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4278\n",
      "Epoch 82: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4278 - val_loss: 0.4479 - lr: 1.0000e-20\n",
      "Epoch 83/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4254\n",
      "Epoch 83: val_loss did not improve from 0.44789\n",
      "85/85 [==============================] - 135s 2s/step - loss: 0.4254 - val_loss: 0.4479 - lr: 1.0000e-20\n",
      "Epoch 84/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4273\n",
      "Epoch 84: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4273 - val_loss: 0.4479 - lr: 1.0000e-20\n",
      "Epoch 85/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4267\n",
      "Epoch 85: val_loss did not improve from 0.44789\n",
      "85/85 [==============================] - 135s 2s/step - loss: 0.4267 - val_loss: 0.4479 - lr: 1.0000e-20\n",
      "Epoch 86/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4266\n",
      "Epoch 86: val_loss did not improve from 0.44789\n",
      "85/85 [==============================] - 135s 2s/step - loss: 0.4266 - val_loss: 0.4479 - lr: 1.0000e-20\n",
      "Epoch 87/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4266\n",
      "Epoch 87: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4266 - val_loss: 0.4479 - lr: 1.0000e-21\n",
      "Epoch 88/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4271\n",
      "Epoch 88: val_loss did not improve from 0.44789\n",
      "85/85 [==============================] - 135s 2s/step - loss: 0.4271 - val_loss: 0.4479 - lr: 1.0000e-21\n",
      "Epoch 89/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4256\n",
      "Epoch 89: val_loss did not improve from 0.44789\n",
      "85/85 [==============================] - 135s 2s/step - loss: 0.4256 - val_loss: 0.4479 - lr: 1.0000e-21\n",
      "Epoch 90/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4247\n",
      "Epoch 90: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4247 - val_loss: 0.4479 - lr: 1.0000e-21\n",
      "Epoch 91/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4266\n",
      "Epoch 91: val_loss did not improve from 0.44789\n",
      "85/85 [==============================] - 135s 2s/step - loss: 0.4266 - val_loss: 0.4479 - lr: 1.0000e-21\n",
      "Epoch 92/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4248\n",
      "Epoch 92: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4248 - val_loss: 0.4479 - lr: 1.0000e-22\n",
      "Epoch 93/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4261\n",
      "Epoch 93: val_loss did not improve from 0.44789\n",
      "85/85 [==============================] - 135s 2s/step - loss: 0.4261 - val_loss: 0.4479 - lr: 1.0000e-22\n",
      "Epoch 94/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4253\n",
      "Epoch 94: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4253 - val_loss: 0.4479 - lr: 1.0000e-22\n",
      "Epoch 95/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4254\n",
      "Epoch 95: val_loss did not improve from 0.44789\n",
      "85/85 [==============================] - 135s 2s/step - loss: 0.4254 - val_loss: 0.4479 - lr: 1.0000e-22\n",
      "Epoch 96/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4258\n",
      "Epoch 96: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4258 - val_loss: 0.4479 - lr: 1.0000e-22\n",
      "Epoch 97/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4263\n",
      "Epoch 97: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 136s 2s/step - loss: 0.4263 - val_loss: 0.4479 - lr: 1.0000e-23\n",
      "Epoch 98/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4263\n",
      "Epoch 98: val_loss did not improve from 0.44789\n",
      "85/85 [==============================] - 135s 2s/step - loss: 0.4263 - val_loss: 0.4479 - lr: 1.0000e-23\n",
      "Epoch 99/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.4250\n",
      "Epoch 99: val_loss improved from 0.44789 to 0.44789, saving model to E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)fileshuman_detection_100_epochs.h5\n",
      "85/85 [==============================] - 137s 2s/step - loss: 0.4250 - val_loss: 0.4479 - lr: 1.0000e-23\n",
      "Epoch 100/100\n",
      "31/85 [=========>....................] - ETA: 1:21 - loss: 0.4325"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data = val_dataset,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fdd9a8-9c04-4aef-af0f-5a9f25fd79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Human_detection_100_epochs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd7cc0f9-e44e-49c1-b028-9f4375909f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_iou(y_true, y_pred):\n",
    "    X1 = max(y_true[0], y_pred[0])\n",
    "    y1 = max(y_true[1], y_pred[1])\n",
    "    X2 = max(y_true[2], y_pred[2])\n",
    "    y2 = max(y_true[3], y_pred[3])\n",
    "\n",
    "    intersection_area = max(0, X2-X1+1) * max(0, y2 - y1+1)\n",
    "\n",
    "    true_area = (y_true[2] - y_true[0] + 1) * (y_true[3] - y_true[1] + 1)\n",
    "    bbox_area = (y_pred[2] - y_pred[0] + 1) * (y_pred[3] - y_pred[1] + 1)\n",
    "\n",
    "    iou = intersection_area / float(true_area + bbox_area - intersection_area)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a53ea92-21c1-4a9d-b05a-f46b1c6aadfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 679 - 679\n",
      "Val : 84 - 84\n",
      "Test : 84 - 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/84 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1466709270977-7b387d9d3471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                  | 1/84 [00:07<10:04,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1518182457238-aacf9536971d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▉                                                                                 | 2/84 [00:07<04:33,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1479849579573-e1d76a5449b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▉                                                                                | 3/84 [00:08<02:46,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1533857474685-882873951157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▉                                                                               | 4/84 [00:08<01:55,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1599140458985-d80aad8b06f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▉                                                                              | 5/84 [00:09<01:28,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1525134479668-1bee5c7c6845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▉                                                                             | 6/84 [00:10<01:12,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1571090308606-124919adff74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▉                                                                            | 7/84 [00:10<01:01,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1590895178913-3d3472310a47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▉                                                                           | 8/84 [00:11<00:54,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1563620915-8478239e9aab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▉                                                                          | 9/84 [00:11<00:51,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1615934679271-1810698dfdfb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▊                                                                        | 10/84 [00:12<00:47,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1644982647708-0b2cc3d910b7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▋                                                                       | 11/84 [00:12<00:45,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1529252233991-9ea40cd5ab41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████▋                                                                      | 12/84 [00:13<00:43,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1517265035603-faefa167335b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▋                                                                     | 13/84 [00:13<00:42,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1523630713882-fbcc8f919a82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▋                                                                    | 14/84 [00:14<00:41,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1527631746610-bca00a040d60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████▋                                                                   | 15/84 [00:15<00:40,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1530631542809-a34985ce72ed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████▌                                                                  | 16/84 [00:15<00:39,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1523319244021-da2f2196547a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                 | 17/84 [00:16<00:38,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1519058082700-08a0b56da9b4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████████████████▌                                                                | 18/84 [00:16<00:38,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1541715877453-1b035764da98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████▌                                                               | 19/84 [00:17<00:38,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1542295856-082da537cda4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████▌                                                              | 20/84 [00:18<00:37,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1582733979150-f7c863d2fd9d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████▌                                                             | 21/84 [00:18<00:35,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1466916119434-d72cdf577c4d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████████████████▍                                                            | 22/84 [00:19<00:35,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1579453316158-ab4918060bee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████▍                                                           | 23/84 [00:19<00:35,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1469395013119-ca3b424d83e5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████████████████▍                                                          | 24/84 [00:20<00:34,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1522845015757-50bce044e5da\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▍                                                         | 25/84 [00:20<00:33,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1532788653456-ac442c9b78d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████████▍                                                        | 26/84 [00:21<00:31,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1523704665369-bd8492bae4ee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████▎                                                       | 27/84 [00:21<00:32,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1492567291473-fe3dfc175b45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████▎                                                      | 28/84 [00:22<00:31,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1610216705422-caa3fcb6d158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████████████████████▎                                                     | 29/84 [00:23<00:30,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1589571894960-20bbe2828d0a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████████████████▎                                                    | 30/84 [00:23<00:30,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1529069535309-8869ac48f10c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████████████████▎                                                   | 31/84 [00:24<00:29,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1533674689012-136b487b7736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████████▏                                                  | 32/84 [00:24<00:28,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1617859822391-9c4bc92ff4f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████▏                                                 | 33/84 [00:25<00:27,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1517344296525-3f2079b011d9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                | 34/84 [00:25<00:27,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1487528001669-63c47a53fd39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████▏                                               | 35/84 [00:26<00:27,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1477591546808-36d4a193cbc2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████▏                                              | 36/84 [00:26<00:26,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1542228935-eb10cfc468a3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████████████████                                              | 37/84 [00:27<00:25,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1580869429356-a07ab2cb25c4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████████████                                             | 38/84 [00:27<00:25,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1558755151-b931c19f89d4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████████████████████████                                            | 39/84 [00:28<00:25,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1524672322836-87e128d1652f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████████████████████████                                           | 40/84 [00:29<00:24,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1570612861542-284f4c12e75f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████████████████████████████████████████                                          | 41/84 [00:29<00:23,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1613667821723-35e4e3e04671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████                                         | 42/84 [00:30<00:22,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1500106067612-d3b2531fc762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████████████████████████▉                                        | 43/84 [00:30<00:23,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1524113844544-cc3b5cc3fb71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████████████████████████▉                                       | 44/84 [00:31<00:21,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1536679815115-54d2bb2c6076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████████████████████████▉                                      | 45/84 [00:31<00:21,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1574099253013-a3521c2e9de0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████████████████████████▉                                     | 46/84 [00:32<00:20,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1523504706857-0b1cc4956993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████████████████▉                                    | 47/84 [00:33<00:21,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1525099089286-22b00686071d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|██████████████████████████████████████████████▊                                   | 48/84 [00:33<00:19,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1542596768-5d1d21f1cf98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████████████████████████▊                                  | 49/84 [00:34<00:19,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1534030347209-467a5b0ad3e6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████▊                                 | 50/84 [00:34<00:19,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1536028009925-1a0d66558f20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████████████████████████████▊                                | 51/84 [00:35<00:18,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1480812494744-bfed1358a9b7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████████████████████████████████████████████████▊                               | 52/84 [00:35<00:17,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1520402787196-ac6264cda85f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|███████████████████████████████████████████████████▋                              | 53/84 [00:36<00:17,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1525360945394-7bccf2809123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████████▋                             | 54/84 [00:36<00:17,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1553993385-8638bf3acd29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████████████████████████████████▋                            | 55/84 [00:37<00:16,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1542409432-9de2c84cdfe6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████▋                           | 56/84 [00:38<00:15,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1534404483017-8743b4e935cd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████████████████████████████████████▋                          | 57/84 [00:38<00:14,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1541342003361-4f01c627e20a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████████████████████████████████▌                         | 58/84 [00:39<00:14,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1612441611175-f101b02c930d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████▌                        | 59/84 [00:39<00:14,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1542663992-7ccaf6d61ad5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████████████████████████████████████████▌                       | 60/84 [00:40<00:13,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1602670895785-e631ecc46e01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████████████▌                      | 61/84 [00:40<00:13,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1460723237483-7a6dc9d0b212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████████████████████████████████████▌                     | 62/84 [00:41<00:12,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1533398892249-1d93b83b958d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████▌                    | 63/84 [00:42<00:11,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1495366691023-cc4eadcc2d7e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████▍                   | 64/84 [00:42<00:11,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1552058544-f2b08422138a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████████▍                  | 65/84 [00:43<00:10,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1611213574794-5e2ea4d6c339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████████████████████████████████████▍                 | 66/84 [00:43<00:10,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1568602471122-7832951cc4c5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▍                | 67/84 [00:44<00:09,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1591001513110-7af0125b6ab8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|██████████████████████████████████████████████████████████████████▍               | 68/84 [00:44<00:08,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1607740527884-7fe5ee67a68a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████████▎              | 69/84 [00:45<00:08,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1615965764209-2b01c8b946a9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████▎             | 70/84 [00:45<00:07,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1608791952180-79294109d843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████████████████████████████████████████▎            | 71/84 [00:46<00:07,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1526065258944-2ebacd27085d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 72/84 [00:47<00:06,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1460639652457-dc0629e460fb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████▎          | 73/84 [00:47<00:06,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1541945871441-1004baea6f8e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████▏         | 74/84 [00:48<00:05,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1543357480-c60d40007a3f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|█████████████████████████████████████████████████████████████████████████▏        | 75/84 [00:48<00:04,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1532703321856-d512f3613d54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▏       | 76/84 [00:49<00:04,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1575767931074-a91868c89acb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████▏      | 77/84 [00:49<00:03,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1527719197793-6b777854108d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████████████▏     | 78/84 [00:50<00:03,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1579295560051-3df968edb036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████     | 79/84 [00:50<00:02,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1580635327346-2ec746c270a2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████████    | 80/84 [00:51<00:02,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1526931208932-14b79dd96623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████████   | 81/84 [00:52<00:01,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1519782296706-fb99bb6f7591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████████████████████████████████████  | 82/84 [00:52<00:01,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1584911883494-14280ff783e4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████████████████████████████████████████████ | 83/84 [00:53<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1606208397452-29faa5b695f5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [00:53<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU: 25.1855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = r\"E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)\"\n",
    "dataset_path = r\"E:\\python\\segmentation\\Computer Vision\\UNET\\data\\objectarchive (2)\"\n",
    "model_path = r\"E:\\python\\segmentation\\Computer Vision\\UNET\\data\"\n",
    "if __name__==\"__main__\":\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    create_dir(data_path + \"\\\\results\")\n",
    "    trained_model = load_model(os.path.join(model_path, \"objectarchive (2)fileshuman_detection_100_epochs.h5\"), compile=False)\n",
    "\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_data(dataset_path)\n",
    "    print(f\"Train : {len(X_train)} - {len(y_train)}\")\n",
    "    print(f\"Val : {len(X_val)} - {len(y_val)}\")\n",
    "    print(f\"Test : {len(X_test)} - {len(y_test)}\")\n",
    "\n",
    "    mean_iou = []\n",
    "    for image, true_bbox in tqdm(zip(X_test, y_test), total=len(X_test)):\n",
    "        name = image.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        print(name)\n",
    "\n",
    "        image = cv2.imread(image, cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (W, H))\n",
    "        x = (image - 127.5) / 127.5\n",
    "        #x = x.astype(np.float32)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "        X1_true, y1_true, X2_true, y2_true = true_bbox\n",
    "                                \n",
    "        pred_bbox = trained_model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        pred_x1 = int(pred_bbox[0] * image.shape[1])\n",
    "        pred_y1 = int(pred_bbox[1] * image.shape[0])\n",
    "        pred_x2 = int(pred_bbox[2] * image.shape[1])\n",
    "        pred_y2 = int(pred_bbox[3] * image.shape[0])\n",
    "\n",
    "        iou = call_iou(true_bbox, [pred_x1, pred_y1, pred_x2, pred_y2])\n",
    "        mean_iou.append(iou)\n",
    "\n",
    "        image = cv2.rectangle(image, (X1_true, y1_true), (X2_true, y2_true), (0, 255, 0), 10)\n",
    "        image = cv2.rectangle(image, (pred_x1, pred_y1), (pred_x2, pred_y2), (0, 0, 255), 10)\n",
    "\n",
    "        x = int(image.shape[1] * 0.05)\n",
    "        y = int(image.shape[0] * 0.05)\n",
    "        font_size = int(image.shape[0] * 0.001)\n",
    "        cv2.putText(image, f\"IoU: {iou:.4f}\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 255, 0), 3)\n",
    "\n",
    "        cv2.imwrite(data_path + f\"\\\\files\\\\{name}.png\", image)\n",
    "\n",
    "    score = np.mean(mean_iou, axis=0)\n",
    "    print(f\"Mean IoU: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d70ee7-7ed8-4324-bfb0-2965358d5811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
